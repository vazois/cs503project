\section{Why mini batch?}

\begin{frame}{Gradient Descent}

\begin{itemize}
  \item{Suppose we have some training set $(x_i,y_i)$ for $i=1,\cdots,N$:}
    \begin{align*}
    \mathcal{L}_{MSE}(\theta)=\frac{1}{N}\sum_{i=1}^{N}(y_i-f(x_i;\theta))^2
    \end{align*}
    \item{Then, the parameters are updated as follow:}
        \begin{align*}
        \theta_j=\theta_j - \alpha\frac{\partial\mathcal{L}_{MSE}(\theta)}{\partial\theta_j}
        \end{align*}
        \end{itemize}
\end{frame}
        
\begin{frame}{Batch vs. Stochastic}  
  \begin{itemize}
    \item{If the gradient is computed by using whole dataset ($N$), it is Batch Gradient Descent. }
	\begin{itemize}
	\item {This is great for convex, or relatively smooth error manifolds.}
	\item{Gradient is less noisy (averaged over a large number of samples).}
	\item{For large or infinite datasets, batch gradient is impractical.}
	%\item{It is able to use optimized matrix operations to efficiently compute output and gradient over whole dataset.}
	\end{itemize}
	\item{Stochastic Gradient Descent(SGD) computes the gradient using a single example.} 	    
	\begin{itemize}
	\item {SGD works well for error manifolds that have lots of local maxima/minima because the somewhat noisy gradient helps to escape local minima into a region that hopefully is more optimal.}
%	\item{More parameters updates imply faster convergence.}
	\item {SGD may go "zig-zag" to a local minimum because of highly noisy gradient.} 
	\end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{Alternative: Mini Batch}
  \begin{itemize}
    \item{Mini Batch Gradient Descent (MGD) is to compute the gradient against more than one training example at each step.}
    \item{$M$ is the mini batch size.}
  \end{itemize}
  \begin{align*}
    & \mathcal{L}^k_{MSE}(\theta)=\frac{1}{M}\sum_{i=s_k}^{s_k+M}(y_i-f(x_i;\theta))^2\\    
       & \theta_k=\theta_k - \alpha\frac{\partial\mathcal{L}^k_{MSE}(\theta)}{\partial\theta_k}
    \end{align*}
    \begin{itemize}
    \item{Parallelization : Distributing training examples across various processors and letting them compute a partial gradient over their own training examples.}
%        \item{Computing the gradient for many cases simultaneously uses matrix-­‐matrix multiplies which are very efficient, especially on GPUs.}
    \end{itemize}
\end{frame}


