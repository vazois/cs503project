\section{Gradient Descent and Backpropagation}
\label{GD}

As explained in section \ref{sub:ClassProb}, classifiers try to choose their parameters ($\theta$) in order to minimize some pre-specified loss function.
In this work, we will work with the Mean-squared Error loss function as defined in equation \ref{LMSE}.

\subsection{Batch Gradient Descent}
\label{sub:BGD}

To optimize the loss function, we will use the Gradient Descent algorithm which in its most basic form takes the derivative of the cost function w.r.t. all the parameter values and updates the parameters by a value proportional to this gradient and opposite in sign.
In its most basic form the Gradient Descent algorithm is implemented as shown in algorithm \ref{alg:BGD}.
\begin{algorithm}[tb]
   \caption{Batch Gradient Descent}
   \label{alg:BGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D} = \{x^{(i)},y^{(i)}\}_{i=1:N}$, Step Size $\alpha$, Max Epochs $N_{epoch}$
   \STATE {\bfseries Output:} Parameters $\{\theta_k\}_{k=1:K}$
   \STATE
   \STATE Initialize $\theta$ randomly with small real numbers
   \STATE $ep := 0$
   \REPEAT
   \STATE $ep := ep + 1$
   \FOR {$k \in \{1:K\}$}
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} = \frac{1}{N} \sum_{i=1}^N ( y^{(i)} - f(x^{(i)}; \theta)) \frac{\partial f(x^{(i)}; \theta)}{\partial \theta_k}$
   \ENDFOR
   \STATE $\theta_k := \theta_k - \alpha \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \hspace{16pt} \forall k \in \{1:K\}$
   \UNTIL{$ep \geq N_{epoch}$}
\end{algorithmic}
\end{algorithm}

\subsection{Minibatch Gradient Descent}
\label{sub:MGD}

Note that the derivative of $\mathcal{L}_{MSE}$ in algorithm \ref{alg:BGD} requires us to sum over all the training examples.
This makes gradient computation, the biggest bottleneck for many supervised learning tasks on huge data sets.
One solution to this problem is to approximate the gradient with a smaller batch of training examples selected from the full dataset, so that more updates can be made in a smaller amount of time.
The resulting algorithm is called Minibatch Gradient Descent (abbreviated as MGD) and is shown in algorithm \ref{alg:MGD}.
\begin{algorithm}[tb]
   \caption{Minibatch Gradient Descent}
   \label{alg:MGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D} = \{x^{(i)},y^{(i)}\}_{i=1:N}$, Step Size $\alpha$, Max Epochs $N_{epoch}$, Batch Size $B$
   \STATE {\bfseries Output:} Parameters $\{\theta_k\}_{k=1:K}$
   \STATE
   \STATE Initialize $\theta$ randomly with small real numbers
   \STATE $ep := 0$
   \REPEAT
   \STATE $ep := ep + 1$
   \STATE Divide $\mathcal{D}$ into batches $\{B_j\}_{j=1:\frac{N}{B}}$ of size $B$ each
   \FOR {$j \in \{1:\frac{N}{B}\}$}
   \FOR {$k \in \{1:K\}$}
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} = \frac{1}{B} \sum_{i \in B_j} ( y^{(i)} - f(x^{(i)}; \theta)) \frac{\partial f(x^{(i)}; \theta)}{\partial \theta_k}$
   \ENDFOR
   \STATE $\theta_k := \theta_k - \alpha \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \hspace{16pt} \forall k \in \{1:K\}$
   \ENDFOR
   \UNTIL{$ep \geq N_{epoch}$}
\end{algorithmic}
\end{algorithm}

\subsection{Parallel Minibatch Gradient Descent}
\label{sub:PMGD}

Even with minibatches, the gradient computation can still be a bottleneck for most training algorithms.
To mitigate this problem, observe that having the gradient as a sum of partial gradients with respect to individual training examples opens up the possibility of parallelizing the gradient computation efficiently.
This can be done by distributing training examples of a minibatch across many processors, letting them compute a partial gradient over their own training examples and then summing up these partial gradients to get the approximate minibatch gradient.
We use this to parallelize the gradient computation as shown in algorithm \ref{alg:PMGD}.
\begin{algorithm}[tb]
   \caption{Parallel Minibatch Gradient Descent}
   \label{alg:PMGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D} = \{x^{(i)},y^{(i)}\}_{i=1:N}$, Step Size $\alpha$, Max Epochs $N_{epoch}$, Batch Size $B$, Num Threads $T$
   \STATE {\bfseries Output:} Parameters $\{\theta_k\}_{k=1:K}$
   \STATE
   \STATE Initialize $\theta$ randomly with small real numbers
   \STATE $ep := 0$
   \REPEAT
   \STATE $ep := ep + 1$
   \STATE Divide $\mathcal{D}$ into batches $\{B_j\}_{j=1:\frac{N}{B}}$ of size $B$ each
   \FOR {$j \in \{1:\frac{N}{B}\}$}
   \STATE Divide $B_j$ into thread-batches $\{B_{jt}\}_{t=1:\frac{B}{T}}$ of size $\frac{B}{T}$
   \STATE \textbf{Thread `t' is forked:}
   \FOR {$k \in \{1:K\}$}
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \big|_t = \sum_{i \in B_{jt}} ( y^{(i)} - f(x^{(i)}; \theta)) \frac{\partial f(x^{(i)}; \theta)}{\partial \theta_k}$
   \ENDFOR
   \STATE \textbf{Thread `t' joins back 'main'}
   \FOR {$k \in \{1:K\}$}
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} = \frac{1}{B} \sum_t \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \big|_t$
   \ENDFOR
   \STATE $\theta_k := \theta_k - \alpha \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \hspace{16pt} \forall k \in \{1:K\}$
   \ENDFOR
   \UNTIL{$ep \geq N_{epoch}$}
\end{algorithmic}
\end{algorithm}

\subsection{Backpropagation}
\label{sub:BackProp}

It can be seen that a major subroutine in algorithms \ref{alg:BGD}, \ref{alg:MGD} and \ref{alg:PMGD} is the computation of gradient of the loss function for a single training example.

Since the neural network is a very complicated function comprising of many layered operations, the gradient of loss function is not trivial to compute.
For fully connected feedforward neural networks, the gradient is computed by starting from the output layer and backpropagating gradient information for previous layers using the derivative chain-rule.

This procedure results in the famous \textit{backpropagation} algorithm to compute the gradient described briefly as follows:
Let $x$ be a training example with label $y$. Our feedforward neural network has an input layer and $L-1$ fully connected layers with sigmoid activation function. The parameters $\theta$ are all the neural network parameters i.e. $\theta = [W_{1:L}, b_{1:L}]$. We will use the Mean-Squared Error loss function:
\begin{align}
\mathcal{L}_{MSE} ([W_{1:L}, b_{1:L}]) = \frac{1}{N}\sum_{i=1}^N ( y - a_L(x)}; [W_{1:L}, b_{1:L}]))^2
\end{align}
where $a_L(x)$ is the activation of the output layer when the input to the neural network is $x$.