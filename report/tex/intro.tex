\section{Introduction}
\label{Intro}

Artificial neural networks are powerful machine learning tools used in many applications including but not limited to search engines, spam and fraud detection, image classification, diagnostic medicine applications and stock market prediction.

Prior to application, neural networks undergo a training phase which is known to be very computationally intensive. This is primarily because the prevailing neural network architectures are implemented using several hidden layers, each consisting of thousands to millions of neurons in order to generalize well on diverse inputs. In this case, the resulting number of parameters that need to be trained are in the order of millions.

Furthermore, achieving high accuracy requires considering a large number of training examples (usually in the order of millions). For this reason training a neural network is both a data and resource intensive operation. This calls for efforts to parallelize the training process on multi-core and while emphasizing at utilizing the maximum available bandwidth.

Currently Minibatch Gradient Descent (henceforth called MGD) is the most commonly used optimization algorithm used to train neural networks in supervised settings. It is implemented in a layerwise-recursive fashion which is termed \textit{backpropagation} in the context of neural networks.

In this paper, we have implemented parallel backpropagation to train Multi Layer Perceptrons (MLPs) for classification tasks.
The rest of the paper is organized as follows: section \ref{ProbDesc} presents a description of supervised learning tasks and neural networks as classifiers. Section \ref{BackProp} describes the conventional serial backpropagation algorithm, and an approach to parallelization using multiple threads. Section \ref{GPUBackProp} describes implementation of backpropagation on a GPU with CUDA. Section \ref{Exp} describes our datasets and the experiments we performed on them. Section \ref{Results} presents our results obtained for these implementations and analyzes the speedup obtained for various network architectures and increasing problem sizes. It also presents a comparison with the same algorithms implemented using a state-of-the-art neural network library Theano. Finally, we conclude in section \ref{Future} with a discussion of potential applications and future work for this project.