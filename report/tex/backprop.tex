\section{Serial and Parallel Backpropagation}
\label{BackProp}

which in its most naive form is implemented as follows:
\begin{enumerate}
\item Initialize all weights $(\theta)$ randomly with small values close to 0.
\item Repeat until convergence \{
\begin{equation*}
\theta_k := \theta_k - \alpha \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \hspace{16pt} \forall k \in \{1,2,...,K\}
\end{equation*}
\}
\end{enumerate}
Note that the derivative of the loss function generally takes the following form:
\begin{equation*}
\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} = \frac{1}{N} \sum_{i=1}^N ( y_i - f(x_i; \theta)) \frac{\partial f(x_i; \theta)}{\partial \theta_k}
\end{equation*}
Since this requires a summation over all the training examples, the gradient computation is the biggest bottleneck for many supervised learning tasks on huge data sets.

On the other hand, having the gradient as a sum of partial gradients with respect to individual training examples opens up the possibility of parallelizing the gradient computation efficiently.
This can be done by distributing training examples across various processors/machines and letting them compute a partial gradient over their own training examples and then summing up these partial gradients to get the actual gradient.
A good approximation of this process can be obtained by using a mini-batch of training examples per iteration, instead of using the full dataset.

Describe backpropagation serial implementation in detail

Describe platform used \\
Describe backpropagation parallelization using pthreads
