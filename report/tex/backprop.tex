\section{Serial and Parallel Backpropagation}
\label{BackProp}

As explained in section \ref{sub:ClassProb}, classifiers try to choose their parameters ($\theta$) in order to minimize some pre-specified loss function.
In this work, we will work with the Mean-squared Error loss function as defined in equation \ref{LMSE}.

\subsection{Batch Gradient Descent}
\label{sub:BGD}

To optimize the loss function, we will use the Gradient Descent algorithm which in its most basic form takes the derivative of the cost function w.r.t. all the parameter values and updates the parameters by a value proportional to this gradient and opposite in sign.
In its most basic form the Gradient Descent algorithm is implemented as shown in algorithm \ref{alg:BGradDesc}.
\begin{algorithm}[tb]
   \caption{Batch Gradient Descent}
   \label{alg:BGradDesc}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D} = \{x^{(i)},y^{(i)}\}_{i=1:N}$, Step Size $\alpha$, Max Epochs $N_{epoch}$
   \STATE {\bfseries Output:} Parameters ${\theta_k}_\{k=1:K\}$
   \STATE
   \STATE Initialize $\theta$ randomly with small real numbers
   \STATE $ep := 0$
   \REPEAT
   \STATE $ep := ep + 1$
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} = \frac{1}{N} \sum_{i=1}^N ( y_i - f(x_i; \theta)) \frac{\partial f(x_i; \theta)}{\partial \theta_k} \hspace{8pt} \forall k \in \{1:K\}$
   \STATE $\theta_k := \theta_k - \alpha \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \hspace{16pt} \forall k \in \{1:K\}$
   \UNTIL{$ep \geq N_{epoch}$}
\end{algorithmic}
\end{algorithm}

Since this requires a summation over all the training examples, the gradient computation is the biggest bottleneck for many supervised learning tasks on huge data sets.

On the other hand, having the gradient as a sum of partial gradients with respect to individual training examples opens up the possibility of parallelizing the gradient computation efficiently.
This can be done by distributing training examples across various processors/machines and letting them compute a partial gradient over their own training examples and then summing up these partial gradients to get the actual gradient.
A good approximation of this process can be obtained by using a mini-batch of training examples per iteration, instead of using the full dataset.

Describe backpropagation serial implementation in detail

Describe platform used \\
Describe backpropagation parallelization using pthreads
