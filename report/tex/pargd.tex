\section{Parallel Gradient Descent}
\label{ParGD}

Even with minibatches, the gradient computation can still be a bottleneck for most training algorithms.
There are two potential ways to get rid of this problem and we describe them in the subsequent subsections.

\subsection{Parallel Minibatch Gradient Descent}
\label{sub:PMGD}

Observe that the gradient is a sum of partial gradients with respect to the individual training examples.
So one way to parallelize gradient computation could be by distributing training examples of a minibatch across many processes, letting them compute a partial gradient over their own training examples and then summing up these partial gradients to get the approximate minibatch gradient.
This approach results in a procedure shown in algorithm \ref{alg:PMGD}.
\begin{algorithm}[tb]
   \caption{Parallel Minibatch Gradient Descent}
   \label{alg:PMGD}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D} = \{x^{(i)},y^{(i)}\}_{i=1:N}$, Step Size $\alpha$, Max Epochs $N_{epoch}$, Batch Size $B$, Num Threads $T$
   \STATE {\bfseries Output:} Parameters $\{\theta_k\}_{k=1:K}$
   \STATE
   \STATE Initialize $\theta$ randomly with small real numbers
   \STATE $ep := 0$
   \REPEAT
   \STATE $ep := ep + 1$
   \STATE Divide $\mathcal{D}$ into batches $\{B_j\}_{j=1:\frac{N}{B}}$ of size $B$ each
   \FOR {$j \in \{1:\frac{N}{B}\}$}
   \STATE Divide $B_j$ into thread-batches $\{B_{jt}\}_{t=1:\frac{B}{T}}$ of size $\frac{B}{T}$
   \STATE \textbf{Thread `t' is forked:}
   \FOR {$k \in \{1:K\}$}
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \big|_t := \sum_{i \in B_{jt}} ( y^{(i)} - f(x^{(i)}; \theta)) \frac{\partial f(x^{(i)}; \theta)}{\partial \theta_k}$
   \ENDFOR
   \STATE \textbf{Thread `t' joins back 'main'}
   \FOR {$k \in \{1:K\}$}
   \STATE $\frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} := \frac{1}{B} \sum_t \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \big|_t$
   \ENDFOR
   \STATE $\theta_k := \theta_k - \alpha \frac{\partial \mathcal{L}_{MSE}}{\partial \theta_k} \hspace{16pt} \forall k \in \{1:K\}$
   \ENDFOR
   \UNTIL{$ep \geq N_{epoch}$}
\end{algorithmic}
\end{algorithm}

\subsection{Parallelizing Matrix Computations}
\label{sub:ParMatComp}

An alternative way to parallelize the gradient computation process can be by parallelizing individual steps of backpropagation (algorithms \ref{alg:FwdPass} and \ref{alg:BackProp}).
Note that the forward propagation and backpropagation algorithms comprise various matrix vector multiplications which can be individually parallelized across multiple threads of execution.
Parallelized matrix-vector multiplications are already efficiently implemented in various linear algebra libraries and we will use the BLAS library to implement this second method of parallelization.

\subsection{Parallelizing on GPUs}
\label{sub:GPU}

Graphics Processing Units (GPUs) are massively parallel processors for efficiently executing certain operations including but not limited to vector-vector addition, matrix-vector and matrix-matrix multiplication. Moreover, because GPUs consist of many throughput oriented multiprocessors that follow the Single Instruction Multiple Data (SIMD) execution model, they can be very useful for applications that require processing large amount of data.

Backpropagation can be accelerated on a GPU using a series of matrix-matrix multiplication and addition kernels. Our implementation relies on several variations of the tiled matrix-matrix multiplication kernel which is available in CUDA samples. The original problem is decomposed in 5 kernel executions per training example which include the computation of the activation values, the calculation of the output error delta and the hidden layer delta values, the calculation of the derivative activation values and the final summation of weights. In the case of computing the delta values, we included an optimization when the sigmoid function is used which avoids recomputing the activation values by deriving the derivative of the activation function from the values of the feed forward step. Also, when computing the final $\Delta W$ that is used to update the weights for each layer, our initial kernel was incurring many shared memory bank conflicts during computation. To improve performance, we chose to invert the order in which tiles are stored in shared memory. This way only a single bank conflict occurs when loading data into shared memory while there are zero bank conflicts during the computation step.

